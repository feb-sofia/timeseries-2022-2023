---
title: "Forecasting ARIMA processes"
author: "Boyko Amarov"
date: "12/21/2021"
output: html_document
---

```{r}
library(forecast)
```
## Information Criteria

As the model complexity increases, e.g. AR(2) is more complex than AR(1) because it has more parameters and is therefore more flexible, the model can fit more closely to the data.
However, this reduces the effective information used to estimate each parameter of the model (limited information spread over a larger number of coefficients). Furthermore, more complex models
tend to fit the noise in the data rather than to capture a systematic development over time (i.e. signal). This is where information criteria come to help choose a model that balances complexity
versus close fit to the observations.

$$
\text{AIC} = -2\log(\text{Likelihood}) + 2(p + q + k + 1)\\
k = \begin{cases}
1 & \delta \neq 0\\
0 & \delta = 0
\end{cases}
$$

In the above equation $k$ equals one or zero depending on whether the model contains a constant ($\delta$), $p$ and $q$ are the orders of the AR and MV parts, and one is added to the penalty because of the estimation of the $\sigma$ parameter (the standard deviation of the white noise process).

Another commonly used information criterion is the Bayes information criterion (BIC):

$$
\text{BIC} = AIC + (\log(T) - 2)(p + q + k + 1)
$$


## Forecasting with ARMA processes

Let $y_t$ follow a stationary ARMA process. The Wold decomposition states that every stationary ARMA process can be represented as MA($\infty$).

$$
\begin{align}
y_t = \mu + \sum_{j = 0}^{\infty} \psi_j e_{t - j}, \quad e_t \sim WN(\sigma^2)
\end{align}
$$

with the restriction that $\psi_0 = 1$ and that the square sum of the coefficients is finite:

$$
\sum_{j = 0}^{\infty} \psi_j^2 \le \infty.
$$

The latter condition ensures that the process has a finite variance. This representation is of limited practical
use, because it contains an infinite number of parameters (which cannot be estimated with finite data). However,
it helps develop the theoretical properties of all ARMA processes. For example, the variance is really easy
to calculate, because the lags of the white noise process are uncorrelated by assumption:

$$
\begin{align}
    Var(y_t) & = Var(\mu) + Var\left(\sum_{j = 0}^{\infty} \psi_j e_{t - j}\right) \\
             & = 0 + \sum_{j = 0}^{\infty} Var(\psi_j e_{t - j}) \\
             & = \sum_{j = 0}^{\infty} \psi_j^2 Var(e_{t - j}) \\
             & = \sum_{j = 0}^{\infty} \psi_j^2 \sigma^2 \\
             & = \sigma^2 \sum_{j = 0}^{\infty} \psi_j^2 \\
\end{align}
$$







Let

$$
\hat{y}_t(h) = \mu + \sum_{k = 0}^{\infty} \theta_k^{h} e_{t - k}
$$

be a forecast function. The error between the value of the process and the prediction for a $h$-steps forecast is the difference between the value of the process $y_{t + h}$ and the forecast:

$$
y_{t + h} - \hat{y}_{t}(h)
$$

it can be shown, that the forecast function with the smallest expected squared error is the conditional expectation of $y_{t + h}$ given the information up to time $t$.

To obtain an expression for the forecast error process, consider the conditional mean of the
white noise process. We write $E_t$ to denote the conditional mean given the information up to and including time $t$.

$$
\begin{align}
E_t(e_{t + h}) =
\begin{cases}
e_{t + h} & h \leq 0 \\
0 & h > 0
\end{cases}
\end{align}
$$

The Wold representation of $y_{t + h}$ is

$$
y_{t + h} = \mu + \psi_0 e_{t + h} + \psi_1 e_{t + h - 1} + \ldots + \psi_h e_t + \psi_{h + 1} e_{t - 1} + \psi_{h + 1} e_{t - 1} + \ldots
$$

The conditional mean is:

$$
E_t(y_{t + h}) = \mu + E_t(\psi_0 e_{t + h}) + E_t(\psi_1 e_{t + h - 1}) + E_t(\ldots) + E_t(\psi_h e_t) + E_t(\psi_{h + 1} e_{t - 1}) + E_t(\psi_{h + 1} e_{t - 1}) + E_t(\ldots)\\
E_t(y_{t + h}) = \mu + \psi_h e_t + \psi_{h + 1} e_{t - 1} + \psi_{h + 1} e_{t - 1} + \ldots
$$

Therefore, the forecast error for the h-steps forecast is:

$$
\begin{align}
f_t(h) = y_{t + h} - E_t(y_{t + h}) = \psi_0 e_{t + h} + \psi_1 e_{t + h - 1} + \psi_2 e_{t + h - 2} +  \ldots
\end{align}
$$

In order to compute prediction intervals, we need the variance of the forecast errors.
This is especially easy for the 1-step ahead forecast:

$$
f_t(1) = e_{t + 1}
$$

and its variance is simply the variance of e_{t + 1} that we have set to $\sigma^2$ by assumption.

$$
Var(f_t(1)) = Var(e_{t + 1}) = \sigma^2
$$

For a two-steps ahead forecast we get:

$$
f_{t}(2) = e_{t + 2} + \psi_1 e_{t + 1}
$$

As the white noise process $e_t$ is uncorrelated (zero covariances), the variance of the forecast errors is simply the sum of the variances of the terms in the sum:

$$
Var(f_{t}(2)) = Var(e_{t + 2}) + Var(\psi_1 e_{t + 1})\\
Var(f_{t}(2)) = \sigma^2 + \psi_1^2 \sigma^2\\
Var(f_{t}(2)) = (1 + \psi_1^2) \sigma^2
$$

For the 3-steps ahead forecast:

$$
f_{t}(3) = e_{t + 3} + \psi_1 e_{t + 2} + \psi_2 e_{t + 1}\\
Var(f_{t}(3)) = (1 + \psi_1^2 + \psi_2^2)\sigma^2
$$

Continuing this, we can find the variance of the h-steps ahead forecast:

$$
Var(f_t(h)) = (1 + \psi_1^2 + \psi_2^2 + \ldots \psi_{h - 1}^2)\sigma^2
$$

To construct approximate 95% prediction intervals for time $t + h$ given the history of the process up to and including $t$:

$$
\hat{y}_t(h) \pm 1.96 \sqrt{Var(f_t(h))}
$$

The $1.96$ factor is the $0.975$ quantile of the standard normal distribution. We must view these prediction intervals with caution, as these are based on the assumption that the forecast errors are approximately normally distributed, uncorrelated and with a constant variance over time. This prediction interval does not account for the uncertainty of estimation that is inherent in the fitting of any model, including the ARIMA models that we study here.


### Forecasts AR(1)

For a (stationary: $|\alpha| < 1$) AR(1) process:

$$
y_{t} = \delta + \alpha x_{t - 1} + e_t, e_t \sim WN(\sigma^2)
$$

The optimal forecast is the conditional mean of $y_{t + h}$ given the information (history of the process) up to and including $t$. We write $E_t$ to denote this conditional expectation. For a white noise process $e_t$ the conditional expectation for $t + h$ is:

$$
E_t(e_{t + h}) = \begin{cases}
e_{t + h} & h \leq 0 \\
0 & h > 0
\end{cases}
$$

the conditional mean at time $t + h$ is:

$$
\begin{align*}
E_t(y_{t + h}) & = E_t(\delta + \alpha y_{t + h - 1} + e_{t + h}) \\
E_t(y_{t + h}) & = \delta + E_t(\alpha y_{t + h - 1}) \\
E_t(y_{t + h}) & = \delta + \alpha \hat{y}_{t}(h - 1)
\end{align*}
$$

Finally, we obtain:

$$
\hat{y}_t(h) = \delta + \alpha \hat{y}_{t}(h - 1)
$$

We can solve this by substituting recursively:

For $h = 1$:

$$
\hat{y}_{t}(1) = \delta + \alpha \hat{y}_t(0) = \delta + \alpha y_{t}
$$

For $h = 2$:

$$
\begin{align*}
\hat{y}_{t}(2)  & = \delta + \alpha \hat{y}_t(1) = \delta + \alpha (\delta + \alpha \hat{y}_t(0)) \\
& = \delta + \alpha \delta + \alpha ^2 \hat{y}_t(0) \\
& = \delta (1 + \alpha) + \alpha^2 y_{t}
\end{align*}
$$

Continuing with the substitution leads to:

$$
\begin{align*}
\hat{y}_t(h)    & = \delta (1 + \alpha + \alpha^2 + \ldots + \alpha^{h - 1}) + \alpha^h y_{t} \\
& = \frac{1 - \alpha^h}{1 - \alpha}\delta + \alpha^h x_{t}
\end{align*}
$$

To derive the variance of the forecast error it is convenient to use the MA($\infty$) representation
of the AR(1) process (Wold representation).

$$
y_t = \delta + \alpha y_{t - 1} + e_t\\
(1 - \alpha L) y_{t} =  \\
y_{t} = \frac{\delta}{1 - \alpha L} + \frac{e_t}{1 - \alpha L} \\
y_{t} = \frac{\delta}{1 - \alpha} + (1 + \alpha L + \alpha^2 L^2 + \alpha^3 L^3 + \ldots) e_{t}\\
y_{t} = \frac{\delta}{1 - \alpha} + e_t + \alpha e_{t - 1} + \alpha^2 e_{t - 2} + \alpha^3 e_{t - 3} + \ldots \\
y_{t} = \mu + e_t + \alpha e_{t - 1} + \alpha^2 e_{t - 2} + \alpha^3 e_{t - 3} + \ldots
$$

We can use \@ref(eq:forecast-error-wold) to obtain the forecast error for the AR(1) process:

$$
f_t(h) = y_{t + h} - \hat{y}_t(h) = e_{t + h} + \alpha e_{t + h - 1} + \alpha^2 e_{t + h - 1} + \ldots + \alpha^{h - 1} e_{t + 1}
$$

Now it is easy to calculate the variance of the forecast error:

$$
\begin{align}
Var(f_t(h)) & = \sigma^2 + \alpha^2 \sigma^2 + \alpha ^ 4 \sigma^2 + \ldots + \alpha^{2(h - 1)}\sigma^2
            & = (1 + \alpha^2 + \alpha^4 + \ldots \alpha^{2(h - 1)}\sigma^2\\
            & = \frac{1 - \alpha^{2h}}{1 - \alpha^2}\sigma^2
\end{align}
$$


```{r}
x <- arima.sim(n = 100, model = list(ar = 0.5))
fit_AR1 <- Arima(x, order = c(1, 0, 0))
fit_AR1
```
```{r}
forecast(fit_AR1, level = 0.95, h = 1)
```

## Forecasting MA(1)

The moving average process of order one is given by:

$$
y_t = \delta + e_t + \beta e_{1 - 1}
$$

The conditional (given the history up to and includnig $t$) mean of $y_{t + h}$ is:

$$
E_ty_{t + h} = \delta + E_te_{t + h} + \beta E_te_{t + h - 1}
$$

For a one step ahead prediction ($h = 1$):

$$
E_{t}y_{t + 1} = \delta + E_te_{t + 1} + \beta E_t e_{t}
$$

Predictions for periods that exceed the order of the MA process (here we have $q = 1$):

$$
E_ty_{t + h} = \delta
$$


## Forcasting ARIMA(1, 0, 1)

Let us combine the methods used for forcasting the AR(1) and the MA(1) models.

$$
y_{t} = \delta + \alpha y_{t - 1} + \beta e_t
$$

The conditional mean of $y_{t + 1}$ given the history of the process up to and including $t$ is:

$$
\hat{y}_t(1) = E_t(y_{t + 1}) = \delta + \alpha y_t + \beta e_t
$$

The problem here is that $e_t$ is not observable, and therefore we cannot use the equation above directly. Instead, we need to estimate $e_t$ from the forecast errors:

$$
\hat{e}_t = y_{t} - \hat{y}_{t - 1}(1)
$$

For $t = 0$ the forecast is:

$$
\hat{y}_{0}(1) = \delta + \alpha x_0 + \beta e_0\\
$$

For $t = 1$:

$$
\begin{align}
\hat{y}_{1}(1)  & = \delta + \alpha y_1 + \beta e_1 \\
                & = \delta + \alpha y_1 + \beta (y_1 - \hat{y}_{0}(1)) \\
                & = \delta + \alpha y_1 + \beta (y_1 - (\delta + \alpha y_0 + \beta e_0)) \\
                & = \delta (1 - \beta) + (\alpha + \beta)y_1 - \beta \alpha y_0 + \beta^2e_0
\end{align}
$$
We can continue substituting until we finally get an admittedly long expression for the forecast at time $t$.

$$
\hat{y}_{t}(1) = \delta(1 -\beta - \beta^2 - \beta^t) +\\ (\alpha - \beta) y_t - \beta(\alpha + \beta)) y_{t - 1} +\\ \ldots +\\ \beta^{t - 1}(\alpha + \beta) y_1 + \beta^t \alpha y_0 + \beta^{t + 1} e_0
$$

To calculate predictions for more than one period, you can use the recursive relationship:

$$
\hat{y}_t(2) = \delta + \alpha \hat{y}_t(1)\\
\hat{y}_t(3) = \delta + \alpha \hat{y}_t(2)\\
\dots\\
$$

## Forecasting ARIMA processes

If the process is ARIMA with $d = 1$, then we need to de-difference the forecast:

$$
\hat{y}_t(h) = y_t + \Delta \hat{y}_{t}(1) + \Delta \hat{y}_{t}(2) + \ldots + \Delta \hat{y}_{t}(h)
$$

If the process is ARIMA with $d = 2$:

$$
\hat{y}_t(h) = y_t + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1)\right) + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1) \Delta^2 \hat{y}_t(2)\right) + \ldots + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1) \Delta^2 \hat{y}_t(2) + \ldots + \Delta^2\hat{y}_t(h)\right)
$$


## Example from the lectures

ARIMA(1, 1, 1):

$$
\Delta y_t = 0.1 \Delta y_{t−1} + e_t + 0.2 e_{t−1}
$$

Compute the one-step ahead forecast of the model given the values

| t   | y   | $x _t = \Delta y_{t}$ | $\hat{x}_{t - 1}(1)$ | $\hat{e}_{t}$ |
|-----|-----|-----------------------|----------------------|---------------|
| -1  |     |                       |                      |               |
| 0   | -11 |                       |                      |               |
| 1   | 2   |                       |                      |               |
| 2   | 5   |                       |                      |               |
| 3   | -1  |                       |                      |               |
| 4   | 13  |                       |                      |               |
| T=5 | 4   |                       |                      |               |
| 6   | ?   |                       |                      |               |
| 7   | ?   |                       |                      |               |


We want to calculate the prediction for periods 6 and 7, given the history of the series up to and including $t = 5$

The conditional expectation of the difference series for period $T + 1 = 6$ is:

For the ease of notation, let us set $x_t = \Delta y_{t}$. The model then becomes:

$$
x_t = 0.1 x_{t - 1} + e_t + 0.2 e_{t−1}
$$

$$
E_T(x_{T + 1}) = E_T(0.1 x_T + e_{T + 1} + 0.2 e_{T})\\
E_T(x_{T + 1}) = 0.1 x_T + 0 + 0.2 \hat{e}_{T})
$$

where $\hat{e}_T = x_T - \hat{x}_{T - 1}(1)$ is the residual from the last observed period $T = 5$. To obtain that residual, write the one period ahead forecast for period $t = 0$.

$$
\hat{x}_{0}(1) = 0.1 x_{0} + 0.2 e_{0}
$$

Next, write the next one period ahead forecast:

$$
\hat{x}_{1}(1) = 0.1 x_1 + 0.2 e_{1} = 0.1 x_1 + 0.2 (x_1 - \hat{x}_{0}(1))\\
$$

Finally, we need to de-difference the forecast:

$$
\hat{y}_{t}(1) = y_{t} + 0.1 \Delta y_{t - 1} + 0.2
$$
