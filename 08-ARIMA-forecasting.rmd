---
title: "Forecasting ARIMA processes"
author: "Boyko Amarov"
date: "12/21/2021"
output: html_document
---

```{r}
library(forecast)
```

## Forecasting with ARMA processes

Let $y_t$ follow a stationary ARMA process. The Wold decomposition states that every stationary ARMA process can be represented as MA($\infty$).

$$
\begin{align}
y_t = \mu + \sum_{j = 0}^{\infty} \psi_j e_{t - j}, \quad e_t \sim WN(\sigma^2)
\end{align}
$$

with the restriction that $\psi_0 = 1$ and that the square sum of the coefficients is finite:

$$
\sum_{j = 0}^{\infty} \psi_j^2 \le \infty.
$$

The latter condition ensures that the process has a finite variance. This representation is of limited practical
use, because it contains an infinite number of parameters (which cannot be estimated with finite data). However,
it helps develop the theoretical properties of all ARMA processes. For example, the variance is really easy
to calculate, because the lags of the white noise process are uncorrelated by assumption:

$$
\begin{align}
Var(y_t) & = Var(\mu) + Var\left(\sum_{j = 0}^{\infty} \psi_j e_{t - j}\right) \\
& = 0 + \sum_{j = 0}^{\infty} Var(\psi_j e_{t - j}) \\
& = \sum_{j = 0}^{\infty} \psi_j^2 Var(e_{t - j}) \\
& = \sum_{j = 0}^{\infty} \psi_j^2 \sigma^2 \\
& = \sigma^2 \sum_{j = 0}^{\infty} \psi_j^2 \\
\end{align}
$$







Let

$$
\hat{y}_t(h) = \mu + \sum_{k = 0}^{\infty} \theta_k^{h} e_{t - k}
$$

be a forecast function. The error between the value of the process and the prediction for a $h$-steps forecast is the difference between the value of the process $y_{t + h}$ and the forecast:

$$
y_{t + h} - \hat{y}_{t}(h)
$$

it can be shown, that the forecast function with the smallest expected squared error is the conditional expectation of $y_{t + h}$ given the information up to time $t$.

To obtain an expression for the forecast error process, consider the conditional mean of the
white noise process. We write $E_t$ to denote the conditional mean given the information up to and including time $t$.

$$
\begin{align}
E_t(e_{t + h}) =
\begin{cases}
e_{t + h} & h \leq 0 \\
0 & h > 0
\end{cases}
\end{align}
$$

The Wold representation of $y_{t + h}$ is

$$
y_{t + h} = \mu + \psi_0 e_{t + h} + \psi_1 e_{t + h - 1} + \ldots + \psi_h e_t + \psi_{h + 1} e_{t - 1} + \psi_{h + 1} e_{t - 1} + \ldots
$$

The conditional mean is:

$$
E_t(y_{t + h}) = \mu + E_t(\psi_0 e_{t + h}) + E_t(\psi_1 e_{t + h - 1}) + E_t(\ldots) + E_t(\psi_h e_t) + E_t(\psi_{h + 1} e_{t - 1}) + E_t(\psi_{h + 1} e_{t - 1}) + E_t(\ldots)\\
E_t(y_{t + h}) = \mu + \psi_h e_t + \psi_{h + 1} e_{t - 1} + \psi_{h + 1} e_{t - 1} + \ldots
$$

Therefore, the forecast error for the h-steps forecast is:

$$
\begin{align}
f_t(h) = y_{t + h} - E_t(y_{t + h}) = \psi_0 e_{t + h} + \psi_1 e_{t + h - 1} + \psi_2 e_{t + h - 2} +  \ldots
\end{align}
$$

In order to compute prediction intervals, we need the variance of the forecast errors.
This is especially easy for the 1-step ahead forecast:

$$
f_t(1) = e_{t + 1}
$$

and its variance is simply the variance of e_{t + 1} that we have set to $\sigma^2$ by assumption.

$$
Var(f_t(1)) = Var(e_{t + 1}) = \sigma^2
$$

For a two-steps ahead forecast we get:

$$
f_{t}(2) = e_{t + 2} + \psi_1 e_{t + 1}
$$

As the white noise process $e_t$ is uncorrelated (zero covariances), the variance of the forecast errors is simply the sum of the variances of the terms in the sum:

$$
Var(f_{t}(2)) = Var(e_{t + 2}) + Var(\psi_1 e_{t + 1})\\
Var(f_{t}(2)) = \sigma^2 + \psi_1^2 \sigma^2\\
Var(f_{t}(2)) = (1 + \psi_1^2) \sigma^2
$$

For the 3-steps ahead forecast:

$$
f_{t}(3) = e_{t + 3} + \psi_1 e_{t + 2} + \psi_2 e_{t + 1}\\
Var(f_{t}(3)) = (1 + \psi_1^2 + \psi_2^2)\sigma^2
$$

Continuing this, we can find the variance of the h-steps ahead forecast:

$$
Var(f_t(h)) = (1 + \psi_1^2 + \psi_2^2 + \ldots \psi_{h - 1}^2)\sigma^2
$$

To construct approximate 95% prediction intervals for time $t + h$ given the history of the process up to and including $t$:

$$
\hat{y}_t(h) \pm 1.96 \sqrt{Var(f_t(h))}
$$

The $1.96$ factor is the $0.975$ quantile of the standard normal distribution. We must view these prediction intervals with caution, as these are based on the assumption that the forecast errors are approximately normally distributed, uncorrelated and with a constant variance over time. This prediction interval does not account for the uncertainty of estimation that is inherent in the fitting of any model, including the ARIMA models that we study here.


### Forecasts AR(1)

For a (stationary: $|\alpha| < 1$) AR(1) process:

$$
y_{t} = \delta + \alpha x_{t - 1} + e_t, e_t \sim WN(\sigma^2)
$$

The optimal forecast is the conditional mean of $y_{t + h}$ given the information (history of the process) up to and including $t$. We write $E_t$ to denote this conditional expectation. For a white noise process $e_t$ the conditional expectation for $t + h$ is:

$$
E_t(e_{t + h}) = \begin{cases}
e_{t + h} & h \leq 0 \\
0 & h > 0
\end{cases}
$$

the conditional mean at time $t + h$ is:

$$
\begin{align*}
E_t(y_{t + h}) & = E_t(\delta + \alpha y_{t + h - 1} + e_{t + h}) \\
E_t(y_{t + h}) & = \delta + E_t(\alpha y_{t + h - 1}) \\
E_t(y_{t + h}) & = \delta + \alpha \hat{y}_{t}(h - 1)
\end{align*}
$$

Finally, we obtain:

$$
\hat{y}_t(h) = \delta + \alpha \hat{y}_{t}(h - 1)
$$

We can solve this by substituting recursively:

For $h = 1$:

$$
\hat{y}_{t}(1) = \delta + \alpha \hat{y}_t(0) = \delta + \alpha y_{t}
$$

For $h = 2$:

$$
\begin{align*}
\hat{y}_{t}(2)  & = \delta + \alpha \hat{y}_t(1) = \delta + \alpha (\delta + \alpha \hat{y}_t(0)) \\
& = \delta + \alpha \delta + \alpha ^2 \hat{y}_t(0) \\
& = \delta (1 + \alpha) + \alpha^2 y_{t}
\end{align*}
$$

Continuing with the substitution leads to:

$$
\begin{align*}
\hat{y}_t(h)    & = \delta (1 + \alpha + \alpha^2 + \ldots + \alpha^{h - 1}) + \alpha^h y_{t} \\
& = \frac{1 - \alpha^h}{1 - \alpha}\delta + \alpha^h x_{t}
\end{align*}
$$

To derive the variance of the forecast error it is convenient to use the MA($\infty$) representation
of the AR(1) process (Wold representation).

$$
y_t = \delta + \alpha y_{t - 1} + e_t\\
(1 - \alpha L) y_{t} =  \\
y_{t} = \frac{\delta}{1 - \alpha L} + \frac{e_t}{1 - \alpha L} \\
y_{t} = \frac{\delta}{1 - \alpha} + (1 + \alpha L + \alpha^2 L^2 + \alpha^3 L^3 + \ldots) e_{t}\\
y_{t} = \frac{\delta}{1 - \alpha} + e_t + \alpha e_{t - 1} + \alpha^2 e_{t - 2} + \alpha^3 e_{t - 3} + \ldots \\
y_{t} = \mu + e_t + \alpha e_{t - 1} + \alpha^2 e_{t - 2} + \alpha^3 e_{t - 3} + \ldots
$$

We can use \@ref(eq:forecast-error-wold) to obtain the forecast error for the AR(1) process:

$$
f_t(h) = y_{t + h} - \hat{y}_t(h) = e_{t + h} + \alpha e_{t + h - 1} + \alpha^2 e_{t + h - 1} + \ldots + \alpha^{h - 1} e_{t + 1}
$$

Now it is easy to calculate the variance of the forecast error:

$$
\begin{align}
Var(f_t(h)) & = \sigma^2 + \alpha^2 \sigma^2 + \alpha ^ 4 \sigma^2 + \ldots + \alpha^{2(h - 1)}\sigma^2
& = (1 + \alpha^2 + \alpha^4 + \ldots \alpha^{2(h - 1)}\sigma^2\\
& = \frac{1 - \alpha^{2h}}{1 - \alpha^2}\sigma^2
\end{align}
$$


```{r}
x <- arima.sim(n = 100, model = list(ar = 0.5))
fit_AR1 <- Arima(x, order = c(1, 0, 0))
fit_AR1
```
```{r}
forecast(fit_AR1, level = 0.95, h = 1)
```

## Forecasting MA(1)

The moving average process of order one is given by:

$$
y_t = \delta + e_t + \beta e_{1 - 1}
$$

The conditional (given the history up to and includnig $t$) mean of $y_{t + h}$ is:

$$
E_ty_{t + h} = \delta + E_te_{t + h} + \beta E_te_{t + h - 1}
$$

For a one step ahead prediction ($h = 1$):

$$
E_{t}y_{t + 1} = \delta + E_te_{t + 1} + \beta E_t e_{t}
$$

Predictions for periods that exceed the order of the MA process (here we have $q = 1$):

$$
E_ty_{t + h} = \delta
$$


## Forcasting ARIMA(1, 0, 1)

Let us combine the methods used for forcasting the AR(1) and the MA(1) models.

$$
y_{t} = \delta + \alpha y_{t - 1} + \beta e_t
$$

The conditional mean of $y_{t + 1}$ given the history of the process up to and including $t$ is:

$$
\hat{y}_t(1) = E_t(y_{t + 1}) = \delta + \alpha y_t + \beta e_t
$$

The problem here is that $e_t$ is not observable, and therefore we cannot use the equation above directly. Instead, we need to estimate $e_t$ from the forecast errors:

$$
\hat{e}_t = y_{t} - \hat{y}_{t - 1}(1)
$$

For $t = 0$ the forecast is:

$$
\hat{y}_{0}(1) = \delta + \alpha x_0 + \beta e_0\\
$$

For $t = 1$:

$$
\begin{align}
\hat{y}_{1}(1)  & = \delta + \alpha y_1 + \beta e_1 \\
& = \delta + \alpha y_1 + \beta (y_1 - \hat{y}_{0}(1)) \\
& = \delta + \alpha y_1 + \beta (y_1 - (\delta + \alpha y_0 + \beta e_0)) \\
& = \delta (1 - \beta) + (\alpha + \beta)y_1 - \beta \alpha y_0 + \beta^2e_0
\end{align}
$$
We can continue substituting until we finally get an admittedly long expression for the forecast at time $t$.

$$
\hat{y}_{t}(1) = \delta(1 -\beta - \beta^2 - \beta^t) +\\ (\alpha - \beta) y_t - \beta(\alpha + \beta)) y_{t - 1} +\\ \ldots +\\ \beta^{t - 1}(\alpha + \beta) y_1 + \beta^t \alpha y_0 + \beta^{t + 1} e_0
$$

To calculate predictions for more than one period, you can use the recursive relationship:

$$
\hat{y}_t(2) = \delta + \alpha \hat{y}_t(1)\\
\hat{y}_t(3) = \delta + \alpha \hat{y}_t(2)\\
\dots\\
$$

## Forecasting ARIMA processes

If the process is ARIMA with $d = 1$, then we need to de-difference the forecast:

$$
\hat{y}_t(h) = y_t + \Delta \hat{y}_{t}(1) + \Delta \hat{y}_{t}(2) + \ldots + \Delta \hat{y}_{t}(h)
$$

If the process is ARIMA with $d = 2$:

$$
\hat{y}_t(h) = y_t + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1)\right) + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1) \Delta^2 \hat{y}_t(2)\right) + \ldots + \left(\Delta y_t + \Delta^2 \hat{y}_{t}(1) \Delta^2 \hat{y}_t(2) + \ldots + \Delta^2\hat{y}_t(h)\right)
$$


## Example from the lectures: ARIMA(1, 1, 1)

$$
\Delta y_t = 0.1 \Delta y_{t−1} + e_t + 0.2 e_{t−1}
$$

Compute the one-step ahead forecast of the model given the values

| t   | y   | $x _t = \Delta y_{t}$ | $\hat{x}_{t - 1}(1)$                            | $\hat{e}_{t}$            |
|-----|-----|-----------------------|-------------------------------------------------|--------------------------|
| -1  |     |                       |                                                 |                          |
| 0   | -11 | NA                    |                                                 |                          |
| 1   | 2   | 13                    |                                                 | $0$                      |
| 2   | 5   | 3                     | $0.1\cdot(13) + 0.2\cdot 0 = 1.3$               | $3 - 1.3 = 1.7$          |
| 3   | -1  | -6                    | $0.1\cdot(3) + 0.2\cdot 1.7 = 0.64$             | $-6 - 0.64 = -6.64$      |
| 4   | 13  | 14                    | $0.1\cdot(-6) + 0.2\cdot (-6.64) = -1.928$      | $14 - (-1.928) = 15.928$ |
| T=5 | 4   | -9                    | $0.1\cdot(14) + 0.2\cdot (15.928) = 4.5856$     | $-9 - 4.5856 = -13.5856$ |
| 6   | ?   |                       | $0.1\cdot(-9) + 0.2\cdot (-13.5856) = -3.61712$ | $0$                      |
| 7   | ?   |                       | $0.1\cdot(-3.61712) + 0.2\cdot 0 = -0.361712$   |                          |

To calculate the forecast, start with the observation at t = 2. For the observations before that there are no available values. The optimal forecast is given by the conditional mean of the series:

$$
\hat{x}_{t}(1) = E_{t}(x_{t + 1}) = E_{t}(0.1 x_t + e_{t + 1} + 0.2 e_{t}) \\
\hat{x}_{t}(1) = E_{t}(x_{t + 1}) = 0.1 x_t + 0.2 \hat{e}_{t}
$$

Let's calculate the one step ahead forecast for $x_2$:

$$
\hat{x}_{1}(1) = E_{1}(x_{2}) = 0.1 x_t + 0.2 \hat{e}_{1}
$$

For the next steps we will estimate $e_{t}$ from the forecast error but for the initial values (where we cannot compute the residual) we will set $e_{1} = 0$.

$$
\hat{x}_{1}(1) = 0.1 \cdot 13 + 0.2 \cdot 0 = 1.3
$$

The forecast for $x_3$ is:

$$
\hat{x}_{2}(1) = 0.1 x_{2} + 0.2 \hat{e}_{2}\\
\hat{e}_{2} = x_2 - \hat{x}_1(1) = 3 - 1.3 = 1.7 \\
\hat{x}_{2}(1) = 0.1 \cdot 3 + 0.2 1.7 = 0.64 \\
$$

We repeat this calculation until we get to the final step. Note that when forecasting $x_7$ the
forecast equation is:

$$
\hat{x}_{6}(1) = 0.1 \hat{x}_{6} = −0.36
$$

To get the prediction for $y_5$ and $y_6$, use apply the formula for reversing the differencing ($d = 1$):

$$
\hat{y}_t(h) = y_t + \Delta \hat{y}_{t}(1) + \Delta \hat{y}_{t}(2) + \ldots + \Delta \hat{y}_{t}(h)
$$

For the one step ahead prediction we obtain $(h = 1)$:

$$
\hat{y}_t(1) = y_t + \hat{x}_t(1) \\
\hat{y}_5(1) = y_{5} + \hat{x}_5(1) = 4 + (−3.61712) = 0.38288
$$

For the two steps prediction we obtain $(h = 2)$

$$
\hat{y}_t(1) = y_t + \hat{x}_t(1) + \hat{x}_t(2) \\
\hat{y}_5(1) = y_{5} + \hat{x}_5(1) + \hat{x}_5(2) = 4 + (−3.61712) + (−0.361712) = 0.021168
$$

We can verify the prediction using `arima` and `predict`. First, we enter the
values by hand and compute the first difference.

```{r}
y <- c(-11, 2, 5, -1, 13, 4)
x <- diff(y)
x
```

 Note that the default estimation method in `arima` is slighly different from what is shown in the table above. To reproduce the results, set the estimation method in `arima` to conditional sum of squares (CSS).

```{r}
fit_ARIMA_1_0_1 <- arima(
  x,
  ## ARIMA(1, 0, 1)
  order = c(1, 0, 1),
  ## Fix the coefficients ar(1) = 0.1, ma(1) = 0.2 , delta = 0
  ## so that they do not get estimated from the data (you'll get different values)
  ## and we want to use exactly the coefficients from the model for calculating the
  ## forecasts
  fixed = c(0.1, 0.2, 0),
  method = "CSS"
)
fit_ARIMA_1_0_1
```
```{r}
residuals(fit_ARIMA_1_0_1)
```
```{r}
predict(fit_ARIMA_1_0_1, n.ahead = 2)
```

You can also do it directly by setting the integration order of the arima process to 1. This time you pass the values of `y` to `arima` and not the differenced values in `x`.

```{r}
fit_ARIMA_1_1_1 <- arima(
  y,
  order = c(1, 1, 1),
  fixed = c(0.1, 0.2),
  method = "CSS"
)
fit_ARIMA_1_1_1
predict(fit_ARIMA_1_1_1, n.ahead = 2)
```

## Example with ARIMA(2, 1, 1)

$$
\Delta y_t = 0.1 \Delta y_{t−1} + 0.3 \Delta y_{t - 2} + e_t + 0.2 e_{t−1}
$$

Again, set $x_t = \Delta y_{t}$ to get a regular ARMA(2, 1) model.

$$
x_t = 0.1 x_{t - 1} + 0.3 x_{t - 2} + e_t + 0.2 e_{t−1}
$$

The optimal forecast is (again) the conditional mean of the process:

$$
\hat{x}_{t}(1) = 0.1 x_t + 0.3 x_{t - 1} + 0.2 \hat{e}_{t}
$$

Your first prediction will be for $t = 3$, because you have no values for $x$ before $t = 1$.

$$
\hat{x}_2(1) = 0.1x_{2} + 0.3 x_{1} + \hat{e}_{2} = 0.1 \cdot 3 + 0.3 \cdot 13 + 0 = 4.2
$$

As you cannot estimate $e_2$ from the residuals, set it to zero. With this estimate, the forecast error is

$$
\hat{e}_{3} = x_3 - \hat{x}_2(1) = -6 - 4.2 = -10.2
$$

You will use this error in the next step to calculate the forecast for $x_4$:

$$
\hat{x}_3(1) = 0.1 x_{3} + 0.3 x_{2} + \hat{e}_3 = 0.1 \cdot (-6) + 0.3 \cdot 3 + 0.2 \cdot (-10.2) = -1.74
$$

You continue this way until you reach the $t = 7$. Then you apply the de-differencing formula as in the previous example.


| t   | y   | $x _t = \Delta y_{t}$ | $\hat{x}_{t - 1}(1)$ | $\hat{e}_{t}$          |
|-----|-----|-----------------------|----------------------|------------------------|
| -1  |     |                       |                      |                        |
| 0   | -11 | NA                    |                      |                        |
| 1   | 2   | 13                    |                      | $0$                    |
| 2   | 5   | 3                     |                      | $0$                    |
| 3   | -1  | -6                    | 4.2                  | $-6 - 4.2 = -10.2$     |
| 4   | 13  | 14                    | -1.74                | $14 - (-1.74) = 15.74$ |
| T=5 | 4   | -9                    |                      | $-11.748$              |
| 6   | ?   |                       |                      | $0$                    |
| 7   | ?   |                       |                      | $0$                    |

```{r}
fit_ARIMA_2_0_1 <- arima(
  x,
  order = c(2, 0, 1),
  fixed = c(0.1, 0.3, 0.2, 0),
  method = "CSS"
)
fit_ARIMA_2_0_1
```
```{r}
residuals(fit_ARIMA_2_0_1)
```
```{r}
predict(fit_ARIMA_2_0_1, n.ahead = 2)
```

## Example with ARIMA(1, 1, 2)

$$
\Delta y_t = 0.1 \Delta y_{t−1} + e_t + 0.2 e_{t−1} + 0.3 e_{t -2}
$$

Substituting $\Delta y_{t} = x_t$:

$$
x_t = 0.1 x_{t−1} + e_t + 0.2 e_{t−1} + 0.3 e_{t -2}
$$

The optimal forecast is given by:

$$
\hat{x}_{t}(1) = 0.1 x_{t} + 0.2 \hat{e}_{t} + 0.3 \hat{e}_{t - 1}
$$

The first prediction for $x_2$ is given by:

$$
\hat{x}_1(1) = 0.1 x_{1} + 0.2 \hat{e}_{1} + 0.3 \hat{e}_{0}
$$

Initializing the forecast error series with zeroes we get:

$$
\hat{x}_1(1) = 0.1 \cdot 13 + 0.2 \cdot 0 + 0.3 \cdot 0 = 1.3 \\
\hat{e}_2 = 3 - 1.3 = 1.7
$$

The next forecast error is:

$$
\hat{x}_2(1) = 0.1 x_{2} + 0.2 \hat{e}_2 + 0.3 \hat{e}_1\\
\hat{x}_2(1) = 0.1 \cdot 3 + 0.2 \cdot 1.7 + 0.3 \cdot 0 = 0.64\\
\hat{e}_3 = -6 - 0.64 = -6.64
$$

and then:

$$
\hat{x}_3(1) = 0.1 x_{3} + 0.2 \hat{e}_3 + 0.3 \hat{e}_2\\
\hat{x}_3(1) = 0.1 \cdot (-6) + 0.2 \cdot (-6.64) + 0.3 \cdot 1.7 = -1.418\\
\hat{e}_4 = 14 - (-1.418) = 15.418
$$


```{r}
fit_ARIMA_1_0_2 <- arima(
  x,
  order = c(1, 0, 2),
  fixed = c(0.1, 0.2, 0.3, 0),
  method = "CSS"
)
fit_ARIMA_1_0_2
```
```{r}
residuals(fit_ARIMA_1_0_2)
```


| t   | y   | $x _t = \Delta y_{t}$ | $\hat{x}_{t - 1}(1)$ | $\hat{e}_{t}$  |
|-----|-----|-----------------------|----------------------|----------------|
| -1  |     |                       |                      |                |
| 0   | -11 | NA                    |                      | $0$            |
| 1   | 2   | 13                    |                      | $0$            |
| 2   | 5   | 3                     | 1.3                  | $1.7$          |
| 3   | -1  | -6                    | 0.64                 | $-6.64$        |
| 4   | 13  | 14                    |                      | $15.418$       |
| T=5 | 4   | -9                    |                      | $-11.4916$     |
| 6   | ?   |                       |                      |                |
| 7   | ?   |                       |                      |                |
