---
title: "Untitled"
author: "Boyko Amarov"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xts)
```

```{r}
set.seed(543)
series1 <- arima.sim(model = list(), n = 400)
series2 <- arima.sim(model = list(ar = c(0.9)), n = 400)
series3 <- arima.sim(model = list(ma = c(0.9)), n = 400)
series4 <- arima.sim(model = list(ar = c(-0.9)), n = 400)
series5 <- arima.sim(model = list(ar = c(0.6, 0.2)), n = 400)
series6 <- arima.sim(model = list(order = c(0, 1, 0)), n = 400)
```

```{r}
pacf(series5)
```


```{r}
series2_l1 <- lag(as.xts(series2))
tmp <- cbind(series2, series2_l1)
?cor
cor(tmp, use = "complete.obs")
```

```{r}
acf(series2, plot = FALSE)
```
```{r}
set.seed(543)
series1 <- arima.sim(model = list(), n = 400)
series2 <- arima.sim(model = list(ar = c(0.9)), n = 400)
series3 <- arima.sim(model = list(ma = c(0.9)), n = 400)
series4 <- arima.sim(model = list(ar = c(-0.9)), n = 400)
series5 <- arima.sim(model = list(ar = c(0.6, 0.2)), n = 400)
series6 <- arima.sim(model = list(order = c(0, 1, 0)), n = 400)
```


$$
y_{t} = y_{t - 1} + e_{t}
$$

```{r}
plot(series6)
acf(series6, lag.max = 50)
pacf(series6)
```


AR(2)

```{r}
plot(series5)
acf(series5)
pacf(series5)
```

AR(1)

$$
y_{t} = \phi_0 -0.9 y_{t - 1} + e_t
$$

$$
\rho_{1} = \phi_1\\
\rho_{2} = \phi_1^2\\
\rho_{3} = \phi_1^3\\
$$

```{r}
plot(series4)
acf(series4)
pacf(series4)
```



MA(1)

```{r}
plot(series3)
acf(series3)
pacf(series3)
```

An invertible MA process has a representation as a AR process of infinite
order.

$$
y_{t} = e_{t} + \theta_1 e_{t - 1}\\
y_{t} = (1 + \theta_1 L)e_{t}\\
\frac{y_{t}}{1 + \theta_1 L} = e_{t}\\
\sum_{k = 0}^{\infty} \theta_1^{k} y_{t - k} = e_{t}\\
y_t = e_{t} - \sum_{k = 1}^{\infty} \theta_1^{k} y_{t - k}
$$


AR(1)

$$
y_{t} = \phi_0 + 0.9 y_{t - 1} + e_t
$$

```{r}
plot(series2)
acf(series2)
pacf(series2)
```

$$
y_{t} = \phi_0 + \phi_1 y_{t - 1} + \phi_2 y_{t - 2} + e_t\\
y_{t - 1} = \phi_0 + \phi_1 y_{t - 2} + \phi_2 y_{t - 3} + e_{t - 1}\\
$$

Direct dependency between $y_t$ and $y_{t - 3}$ in the equation above?



White noise process (purely random process)

```{r}
plot(series1)
acf(series1)
pacf(series1)
```
ARIMA(p, i, q)

ARIMA(2, 0, 2)

$$
y_{t} = \phi_0 + \phi_1 y_{t - 1} + \phi_2 y_{t - 2} + e_t + \theta_1 e_{t - 1} + \theta_2 e_{t - 2}, \quad e_{t} \sim WN(\sigma^2)
$$

ARIMA(0, 0, 0)

$$
y_{t} = \phi_0 + e_{t}
$$

## Fitting ARIMA models

```{r}
plot(series2)
acf(series2)
pacf(series2)
```


$$
y_{t} = \phi_0 + \phi_1 y_{t - 1} + e_t, e_{t} \sim WN(\sigma^2)
$$
```{r}
## ARIMA(1, 0, 0): AR(1)
fit_1 <- arima(series2, order = c(1, 0, 0))
fit_1
```

Estimated equation for y:

$$
\hat{y}_{t} =  0.1 + 0.91 \hat{y}_{t - 1}
$$
What is a "good" model for the data?

Residual series

$$
r_{t} = y_{t} - \hat{y}_{t}
$$
```{r}
tsdiag(fit_1)
```

ARIMA(0, 0, 0)
$$
y_{t} = \phi_0 + e_{t}
$$

```{r}
## ARIMA(0, 0, 0)
fit_bad <- arima(series2, order = c(0, 0, 0))
fit_bad
```

```{r}
tsdiag(fit_bad)
```


```{r}
## ARIMA(0, 0, 0)
fit_too_much <- arima(series2, order = c(20, 0, 0))
fit_too_much
```


```{r}
tsdiag(fit_too_much)
```

## Information Criteria

As the model complexity increases, e.g. AR(2) is more complex than AR(1) because it has more parameters and is therefore more flexible, the model can fit more closely to the data.
However, this reduces the effective information used to estimate each parameter of the model (limited information spread over a larger number of coefficients). Furthermore, more complex models
tend to fit the noise in the data rather than to capture a systematic development over time (i.e. signal). This is where information criteria come to help choose a model that balances complexity
versus close fit to the observations.

$$
\text{AIC} = -2\log(\text{Likelihood}) + 2(p + q + k + 1)\\
k = \begin{cases}
1 & \delta \neq 0\\
0 & \delta = 0
\end{cases}
$$

In the above equation $k$ equals one or zero depending on whether the model contains a constant ($\delta$), $p$ and $q$ are the orders of the AR and MV parts, and one is added to the penalty because of the estimation of the $\sigma$ parameter (the standard deviation of the white noise process).

Another commonly used information criterion is the Bayes information criterion (BIC):

$$
\text{BIC} = AIC + (\log(T) - 2)(p + q + k + 1)
$$

When comparing candidate models, better models have a lower values of the information criteria. Do not use the IC to select the order of integration, as differencing changes the data and the IC values are not comparable!
